% This chapter should describe what was actually produced: the programs which were written, the hardware which was
% built or the theory which was developed. Any design strategies that looked ahead to the testing stage might 
% profitably be referred to (the professional approach again).

% Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left % to appendices or omitted altogether. Analogous advice applies to circuit diagrams.
% Draw attention to the parts of the work which are not your own. Making effective use of powerful tools and 
% pre-existing code is often laudable, and will count to your credit if properly reported.
% It should not be necessary to give a day-by-day account of the progress of the work but major milestones may 
% sometimes be highlighted with advantage.

% ~4500 words
\documentclass[final,rdr32.tex]{subfiles}


\begin{document}

\chapter{Implementation}

In this chapter, the overall implementation of the project will be described, mentioning the challenges faced along the way. The implementation pipeline consists of the following key processes:

\begin{itemize}
    \item \textbf{Recording signs: }This includes the code which uses OpenCV to capture frames. They also integrate most of the functionality for determining when to process a given sequence of frames, when to reset the recording, and displaying visual and textual output to the user.
    \item \textbf{Pre-processing datasets: }These scripts help process the datasets into a suitable format for the program to read them. LSA64 has one script which does the entire processing step, whereas BSLDict has an additional script to filter labels that have less than a desired amount of data.
    \item \textbf{Extracting features from the datasets: }This is the code that is used to obtain the relevant features that will be used to train our model. Mediapipe holistic is used to first extract landmarks from video frames, then to convert these to angles and relative distances. This is discussed further in the relevant following section.
    \item \textbf{Comparing signs: }This part of the pipeline focuses on the computation using the DTW algorithm. It includes the code which returns the distances between any two compared signs and also the functionality to determine if any given sign is output with a certain confidence.
\end{itemize}

There are many scripts which when combined together, achieve those processes. The project makes use of objects to make the code reusable and to easily be able to expand it.

\section{Repository Overview}

This section provides an overview of the source code and how the scripts are organised in the repository. The project has been organised such that it is easy to add new datasets and to train on them, provided a new script to process it is written.
\bigskip

\setlength{\DTbaselineskip}{16pt}
\dirtree{%
    .1 .
    .2 datasets\DTcomment{Folder to store datasets}.
    .2 models.
    .3 hand\_model.py\DTcomment{\autoref{sec:hand_model}\quad\textbf{72 lines}}.
    .3 pose\_model.py\DTcomment{\autoref{sec:pose_model}\quad\textbf{48 lines}}.
    .3 sign\_model.py\DTcomment{\autoref{sec:sign_model}\quad\textbf{76 lines}}.
    .2 utils.
    .3 bsldict\_data\_processing.py\DTcomment{\autoref{sec:data}\quad\textbf{35 lines}}.
    .3 bsldict\_preprocessing.py\DTcomment{\autoref{sec:data}\quad\textbf{36 lines}}.
    .3 compute\_dtw.py\DTcomment{\autoref{sec:dtw-python}\quad\textbf{56 lines}}.
    .3 compute\_fastdtw.py\DTcomment{\autoref{sec:fastdtw}\quad\textbf{45 lines}}.
    .3 constants.py\DTcomment{\autoref{sec:constants}\quad\textbf{11 lines}}.
    .3 feature\_extraction.py\DTcomment{\autoref{sec:feature}\quad\textbf{172 lines}}.
    .3 lsa64\_data\_processing.py\DTcomment{\autoref{sec:data}\quad\textbf{42 lines}}.
    .3 mediapipe\_utils.py\DTcomment{\autoref{sec:mediapipe}\quad\textbf{67 lines}}.
    .3 sign\_eval.py\DTcomment{\autoref{sec:sign_eval}\quad\textbf{45 lines}}.
    .3 sign\_recorder.py\DTcomment{\autoref{sec:sign_recorder}\quad\textbf{120 lines}}.
    .3 webcam\_manager.py\DTcomment{\autoref{sec:webcam}\quad\textbf{71 lines}}.
    .2 eval.py\DTcomment{Script to run test data evaluation\quad\textbf{57 lines}}.
    .2 main.py\DTcomment{Script to run real-time detection\quad\textbf{47 lines}}.
}


\section{Data Pre-processing}
\label{sec:data}

The model needs the data to be in a a required format in order to process it. As such, we need to pre-process existing datasets in order to make them suitable.

We order the videos by having a directory for each label, containing all videos which fall under that corresponding label. This is illustrated as follows:
\bigskip

\dirtree{%
    .1 train\_videos.
    .2 Accept.
    .3 Accept-0.mp4.
    .3 Accept-1.mp4.
    .2 Appear.
    .3 Appear-0.mp4.
    .3 Appear-1.mp4.
}
\bigskip
The scripts \verb|bsldict_data processing.py | and \verb|lsa64_data_processing.py| process their respective datasets to obtain it in such a format. \\
Note that \verb|bsldict_preprocessing.py| is used first if we wish to filter labels which have more than a minimum number of videos.
\section{Mediapipe Extraction}
\label{sec:mediapipe}

Using our datasets, we now need to extract information from them. This is done by processing each frame in a given video individually.

\subsection{Mediapipe Holistic}

Mediapipe Holistic is the model that outputs the needed data to start the feature extraction process.

Mediapipe Holistic utilizes a deep neural network model that is trained on a large dataset of labeled images and videos. The model is designed to recognize various human poses, hand gestures, and facial landmarks, and it achieves this by analyzing the pixels of the input image or video stream \cite{grishchenko_bazarevsky_2020}. Mediapipe Holistic provides three tracking pipelines, namely for the face, hands, and pose. The latter two are of relevance to this project.

\subsection{Hands Detection Pipeline}

Detecting hands with articulated fingers is a complex task, due to factors such as varying hand spans relative to image frames. Unlike faces, hands lack high-contrast areas, increasing the complexity of detecting them. As such, usage of other features such as arms and the torso can help localise hand positions more accurately.

Mediapipe employs palm detection, as boundary-estimation is relatively easier compared to hands with articulated fingers. A Single-Shot Detector model called BlazePalm is used to detect initial hand locations. An encoder-decoder feature extractor is used for bigger scene context awareness, an approach similar to RetinaNet. Further explanation about these methods are described below.

\subsubsection{Single-Shot Detection}

Single Shot MultiBox Detector (SSD) is a state-of-the-art object detection algorithm that was proposed by Liu et al. in 2016 \cite{liu2016ssd}. SSD is a single-shot detector, which means that it can detect objects in a single forward pass through the network. This makes SSD much faster than two-stage detectors like R-CNN and Fast R-CNN, which require two forward passes through the network.

SSD works by predicting a set of bounding boxes and confidence scores for each object in an image. The bounding boxes are predicted using a set of anchor boxes, which are predefined boxes that are placed at different locations and scales in the image. The confidence scores are predicted for each anchor box, indicating how likely it is that the anchor box contains an object.

After the bounding boxes and confidence scores have been predicted, they are used to generate a set of final detections. This is done by first applying non-maximum suppression (NMS) to the bounding boxes. NMS is a technique that is used to remove duplicate detections. In NMS, the bounding boxes are sorted by their confidence scores, and then the boxes that have a high overlap with other boxes are removed.

After NMS has been applied, the remaining boxes are then classified using a softmax classifier. The softmax classifier outputs a probability distribution over the set of object classes. The class with the highest probability is then assigned to the box.

SSD has been shown to be very effective for object detection. It has achieved state-of-the-art results on a number of benchmark datasets, including the PASCAL VOC dataset and the COCO dataset. SSD is also very fast, making it suitable for real-time object detection applications.

\subsubsection{Non-Maximum Suppresion}

Non-maximum suppression (NMS) is a technique used in computer vision to eliminate duplicate or redundant detections of the same object in an image. It is commonly used in object detection tasks such as pedestrian detection, face detection, and vehicle detection. The algorithm works by selecting the detection with the highest confidence score and suppressing any overlapping detections that have a lower confidence score.

The NMS algorithm can be described mathematically as follows:

Given a set of bounding boxes, $B = {b_1, b_2, ..., b_n}$, and their corresponding confidence scores, $S = {s_1, s_2, ..., s_n}$, where $n$ is the number of bounding boxes, the goal is to select a subset of bounding boxes, $B^* \subseteq B$, that have a high confidence score and do not overlap significantly with each other.

First, we sort the bounding boxes in descending order of their confidence scores. Let $b_i$ be the bounding box with the highest confidence score, i.e., $s_i = \max_{j=1}^{n} s_j$. We add $b_i$ to the subset $B^*$ and remove it from the set $B$.

Next, we calculate the Intersection over Union (IoU) between $b_i$ and each of the remaining bounding boxes $b_j \in B$, where $j \neq i$. The IoU is defined as:

\begin{equation*}
    \text{IoU}(b_i, b_j) = \frac{\text{area}(b_i \cap b_j)}{\text{area}(b_i \cup b_j)}
\end{equation*}

If $\text{IoU}(b_i, b_j) > \text{threshold}$, where $\text{threshold}$ is a predetermined threshold value (e.g., 0.5), we remove $b_j$ from the set $B$. We repeat this process until all the bounding boxes in $B$ have been processed.

The final output is the set $B^*$ of selected bounding boxes.

The NMS algorithm helps to reduce the number of false positives and improves the accuracy of object detection models by removing redundant detections.

\subsection{Pose Detection Pipeline}

The process for pose detection, BlazePose, is similar to that used for hands \cite{bazarevsky2020blazepose}. The BlazePose detection pipeline consists of a lightweight pose detector followed by a pose estimation component.

\subsubsection{Pose Detection}

The first stage of the BlazePose pipeline is body part detection. Unlike the hands detection pipeline, BlazePose does not use the NMS algorithm as it breaks down in cases where poses are highly articulated. This is because multiple bounding boxes satisfy the IoU threshold. Instead, a face detector is used as a proxy for the person detector. This is augmented with additional person alignment parameters such as the middle point between the person's hip. The CNN used in the face detector is based on the BlazeFace architecture \cite{bazarevsky2019blazeface}, which uses a modified MobileNetV1 model with depth-wise separable convolutions.

\subsubsection{Pose Estimation}

The pose estimation component predicts the location of the 33 landmarks, using the bounding boxes that were obtained from the first stage.

To achieve accurate pose estimation, a combined heatmap, offset, and regression approach is adopted. During training, the heatmap and offset loss are used to supervise the model, but the corresponding output layers are removed before running inference. This approach effectively uses the heatmap to supervise a lightweight embedding, which is then used by the regression encoder network. This approach is inspired by the Stacked Hourglass approach of Newell et al. \cite{newell2016stacked}, but in this case, a tiny encoder-decoder heatmap-based network is stacked with a subsequent regression encoder network.

To achieve a balance between high- and low-level features, skip-connections are actively utilized between all the stages of the network. However, the gradients from the regression encoder are not propagated back to the heatmap-trained features. This approach not only improves the heatmap predictions but also substantially increases the coordinate regression accuracy.

\subsection{Mediapipe Pipeline}

One problem with using multiple separate models is that each is optimised for their specific purposes \cite{grishchenko_bazarevsky_2020}. For example, the pose estimation model requires a $256 \times 256$ input video frame. However, cropping the hand and face from this frame to feed their respective models results in an input which has a too low resolution to obtain accurate landmarks.

To solve this, Mediapipe Holistic uses a multistage pipeline. First, the human pose is estimated using the pose pipeline. Using the inferred pose landmarks, it then derives the regions of interest (ROI) for the face and hands, employing a recrop model along the way to improve it. The ROIs are then passed into the respective models to obtain the remaining landmarks.

\newpage

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=\textwidth]{images/holistic_pipeline.png}
        \caption[caption]{Pipeline Overview}
    \end{center}
\end{figure}

\subsection{Mediapipe Implementation}

The goal here is to obtain the position of the different connections between joints.
\newline \verb|mediapipe_utils| provides methods which help in locating the landmarks given a frame. The detection is relatively straightfoward to implement, using the provided Holistic model that is imported as part of the library. We can just use the model's \verb|process| method. We also have to do color conversion because frames from OpenCV use BGR color rather than RGB. To add visual representation to the frames, the method \verb|draw_landmarks| can be used to add drawing specifications to display the landmarks and the connections between them.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.3]{images/holistic.jpg}
        \caption[caption]{Visual display of landmarks of Mediapipe Holistic (Image from mediapipe.dev)}
    \end{center}
\end{figure}



\section{Feature Extraction}
\label{sec:feature}

Once a list of connections from mediapipe has been obtained, we want to store this data in a suitable format and being able to reload them. The \verb|feature_extraction| script provides a range of methods to do so. The initial training stage involves obtaining a collection of reference signs. These are what the recorded signs will be compared to to evaluate similarity.

\subsection{Extract to Pickle Files}

Initially, the \verb|extract_features| method is used to obtain the landmarks from the data in our training folder. Every video is scanned and the relevant data extracted by mediapipe is stored in pickle files.

We use this opportunity here to also extract the data of the laterally-inverse of the frames. The reason for this is to obtain the features of the sign performed in a left-handed manner. In BSL, the leading hand used to perform the signing determines the handedness of the sign. Thus, flipping the image is sufficient to obtain the relevant data.

Each training video has two entries corresponding to it: its own extracted data and its flipped data. Each entry contains a pickle file for each hand and the pose, which is essentially an array of landmarks.

\subsection{Loading Reference Signs}

To perform predictions, the reference data needs to be loaded for the session. The \verb|load_reference_signs| method is used to iterate through the entries containing the pickle files. For each entry, we initialise a new \verb|sign_model| object, which is itself initialised with the respective \verb|hand_model| and \verb|pose_model| objects constructed from the data stored in the pickle files.

For efficiency, we use a \verb|pandas.DataFrame| to store the name of the sign, its respective \verb|sign_model|, and a distance initialised to 0. This will be overwritten when computing the distances.

\section{Models}
\label{sec:models}

The project uses three models to represent features extracted from the vectors provided by Mediapipe Holistic.

\subsection{Hand Model}
\label{sec:hand_model}

The \verb|hand_model| class creates objects that store the feature vector representing all the angles between the connections provided by mediapipe on an arbitrary hand. Mediapipe provides 21 connections per hand, resulting in a feature vector of $21 * 21 = 441$ angles. We use the dot product formula to obtain the angle given two vectors (See Figure 3.2):

\begin{equation*}
    \theta = cos^{-1}\Big(\frac{a \cdot b}{|a| |b|}\Big) \text{, where $a$ and $b$ represent connections}
\end{equation*}

The model only represents one particular frame, given a sequence of them. The final sign gesture is thus represented by a sequence of hand models, one for each hand.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.7]{images/hand_landmarks.png}
        \caption{The angles on the mediapipe connection drawing}
    \end{center}
\end{figure}


\subsection{Pose Model}
\label{sec:pose_model}

The \verb|pose_model| takes a different approach. Instead of using angles, we use the distance between the shoulder and elbow as a normalisation distance, normalising coordinates provided by mediapipe. This is because the pose landmarks also include those of the lower body, including the legs. Hence, we only use the landmarks of the shoulder, elbows and wrist, as they are more likely to be present in the frames. Taking $a$ and $b$ as the position vectors representing the shoulder and elbow respectively, we obtain the new coordinates as follows:
\begin{equation*}
    v_i = \frac{u_i}{|b - a|} \text{, for every coordinate $u$ in the pose landmarks}
\end{equation*}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{images/pose.png}
        \caption{The relative distance used to normalise the landmarks}
    \end{center}
\end{figure}

\subsection{Sign Model}
\label{sec:sign_model}

The \verb|sign_model| object stores the hand and pose models for each frame in the sequence representing the sign, essentially being the representation of the sign as its features. This is what is compared when computing the similarity between a reference sign and a recorded sign.

Each object contains the following attributes:

\begin{itemize}
    \item \verb|has_left_hand| and \verb|has_right_hand|, which are booleans which are true if such hand is present in the video.
    \item \verb|lh_embedding| and \verb|rh_embedding|, which are the lists of feature vectors for each frame for each hand.
    \item \verb|left_arm_embedding| and \verb|right_arm_embedding|, which are the lists of feature vectors for each frame for each arm.
\end{itemize}

\section{Computing Similarity}

The project provides two scripts for computing similarity: \verb|compute_dtw| and\newline \verb|compute_fastdtw|, which use the \verb|dtw-python|\cite{giorgino2009computing} and \verb|fastdtw| \cite{salvador2007toward} libraries respectively. The project provides an easy way to toggle between the two algorithms.

\subsection{dtw-python}
\label{sec:dtw-python}

The outline of the DTW algorithm is mentioned in Section 2.3.2. The implementation of the algorithm uses the \verb|dtw| method from the \verb|dtw-python| library, without any additional arguments. The package allows windowing to be used, which can speed up the algorithm, but an alternative algorithm, FastDTW, is proposed later instead.

The \verb|dtw_distances| method is what fills the \verb|pandas.DataFrame| with distances. It takes the DataFrame as an argument along with the sign model representing the recorded sign. The embeddings of the sign model are extracted to variables as they are the sequences to be compared.

The model starts iterating through every sign present in the DataFrame. It discards those which do not have the same hands present in the recorded sign and sets their distances as $\inf$ in the distance column of the DataFrame. The \verb|dtw| method from \verb|dtw-python| \cite{giorgino2009computing} is then called with the respective embeddings of the reference and recorded sign as arguments. The distance obtained is then set in the corresponding entry of the DataFrame.


\subsubsection{Challenges Faced}

The \verb|dtw| method handles the case of having a sequence of length 1 differently. In such a case, it transposes the array. This happened in a few instances on training due to some signs registering a particular hand in only one frame. This is dependent on the confidence tracking of mediapipe. As such, this specific case is handled by duplicating the singular entry present in the sequence to obtain an array of length 2.

\subsection{fastdtw}
\label{sec:fastdtw}
A high-level overview of the FastDTW algorithm is given in Section 2.4.5. This section will provide a detailed overview of the algorithm, as outlined in the paper proposing it \cite{salvador2007toward} and also cover the implementation, which was done using the \verb|fastdtw| library.

\subsubsection{Speeding up DTW}

FastDTW uses ideas from two approaches to speed up the DTW algorithm.

\textit{Constraints} are used to limit the number of cells that are present in the distance matrix. For example, the Sakoe-Chiba band \cite{sakoe1978dynamic} limits the cells to a band shape and the Itakura parallelogram \cite{itakura1975minimum} limits them to a parallelogram with corners at the start and end of the distance matrix.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=1]{images/constraint.png}
        \caption{Sakoe-Chiba Band (left) and Itakura Parallelogram (right)}
    \end{center}
\end{figure}

Only the shaded cells are filled in by the DTW algorithm. We determine the window, or width, of the shaded area using a parameter to the algorithm.

This method does not guarantee that the optimal global alignment will be found, as it only limits the valid warping paths to those within the constraints of the window.

Using constraints improves the run-time of the algorithm by a constant factor, but it still runs in O$(m \times n)$ if the size of the window is a function of the length of the input time series. They work best where the optimal global alignment is expected to be close to a linear warp and passes through the bounded area of the distance matrix.


\textit{Data abstraction} \cite{chu2002iterative} is another way to speed up the algorithm. The aim is to reduce the representation of the data, a smaller matrix in this case, to run the algorithm on.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=1]{images/data_abstraction.png}
        \caption{Finding warp path on lower-resolution distance matrix}
    \end{center}
\end{figure}

Instead of running the algorithm on a full-resolution matrix, the number of points in the time series are reduced, leading to a lower resolution matrix. The algorithm is then used on this matrix to obtain a warp path, which is then mapped back onto the original full-resolution matrix.

The resulting warp path is usually not optimal, and the level of inaccuracy increases the more the matrix is simplified. This can be significant in applications where local variations have a great effect on the warp path.

This method speeds up the algorithm by a constant factor as well, but the overall time and space complexity remain unchanged.

\subsubsection{Algorithm Description}

The algorithm is based on graph bisection \cite{karypis1997multilevel}, which involves splitting a graph into smaller parts, roughly equivalent. The goal is to minimise the sum of the edges that would be broken. A multi-level approach aims at correcting large graphs with the solutions from smaller ones, in a dynamic-programming fashion. FastDTW uses this to find an accurate warp path.

The algorithm involves the following operations:
\begin{enumerate}
    \item \textit{Coarsening}, which involves representing the time series in a reduced form, using as few points as possible.
    \item \textit{Projection}, which is to find the warp path on a lower-resolution matrix. This will be used as an initial baseline for further refinement after projecting it on the full-resolution matrix.
    \item \textit{Refinement}, which is to refine the projected warp path through local adjustments of the warp path.
\end{enumerate}

\textit{Coarsening} is used to generate many different resolutions of the matrix, each of them gradually decreasing by a factor of 2 starting from the original time series. \textit{Projection} is then used to find the warp path on the lowest resolution matrix. This is then projected on the next lowest-resolution matrix. This is represented as the dark-shaded cells in Figure 3.9.

The dark-shaded cells are used as a window for the next run of DTW, being used as a constraint. This does not guarantee that the optimal global path will be present within that constraint, but the accuracy can be increased by increasing the \textit{radius} of the projected warp path. In Figure 3.9, lightly-shaded cells represent those included in the window, with the \textit{radius} parameter set to 1.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=1]{images/fastdtw.png}
        \caption{Gradual refinement of warp path as resolution increases}
    \end{center}
\end{figure}

FastDTW evaluates only the cells in the neighbourhood of the projected warping path. The number of cells that the algorithm computes scales linearly with the length of the time series. This leads to a complexity of $O(n)$, if we consider $n$ to be a representation of both time series being compared.

The following pseudocode, as outlined in the relevant paper \cite{salvador2007toward}, describes the algorithm:\\

\begin{algorithm}[H]
    \caption{FastDTW algorithm}
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}

    \KwIn{X: array $[x_1, x_2, \dots, x_m]$, Y: array $[y_1, y_2, \dots, y_n]$}, radius: int \\
    \KwOut{Distance: float}

    // The minimum size of the coarsest resolution, the base case\\
    $minSize = radius+2$

    \eIf{$|X| \leq minSize$ \textbf{or} $|Y| \leq minSize$}{
        // For small time-series, run DTW\\
        \KwRet{DTW($X$, $Y$)}
    }{
        $reducedX = X.reduceByHalf$\\
        $reducedY = Y.reduceByHalf$\\
        $lowResPath = FastDTW(reducedX, reducedY, radius)$\\
        $window = HighResWindow(lowResPath, X, Y, radius)$\\
        \KwRet{DTW($X$, $Y$, $window$)}
    }
\end{algorithm}
\bigskip
Lines 8-9 are executed multiple times until the base case is reached. Line 10 is a recursive call which updates the window one very call, which is then passed to the DTW algorithm to do the adjustments with the new constraints.

\subsubsection{Implementation}

The \verb|fastdtw_distances| method works similarly to \verb|dtw_distances|. It uses the \verb|fastdtw| method from the \verb|fastdtw| library instead to calculate the distances. This implementation does not suffer from the same issue of handling sequences of length 1. This implementation runs in linear time, rather than in O$(m \times n)$.

\section{Sign Prediction}

Given the \verb|pandas.DataFrame| with all distances computed, a prediction can now be made. The DataFrame is first sorted in ascending order of distances, with the top row being the sign with the lowest distance. Lower distances indicate higher similarity.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=1]{images/dataframe.png}
        \caption{Dataframe with more likely labels, sorted by distance}
    \end{center}
\end{figure}

\subsection{Batch Size and Threshold}
\label{sec:batch}

The \verb|get_sign_predicted| method takes an argument for \verb|batch_size|. This indicates how many signs we need to consider in order to make a reliable prediction. For example, we can take a batch size of 5, which means we take the 5 signs with the lowest distances and output the most common one. This increases the confidence in the prediction made. We choose a label $l$ as follows:

\begin{equation*}
    l = \max\Big(\frac{l_i}{N}\Big) \text{, for $0 \leq i < N$}
\end{equation*}

$N$ corresponds to the chosen \verb|batch_size|. A reasonable upper bound for $N$ can be obtained by considering the label in the training data with the least amount of examples. This ensures that every label has the possibility of achieving a confidence of 100\%.

The threshold argument is used to determine if a sign is common enough to be output with a certain minimum confidence. It is defined as the minimum ratio of appearances to the batch size that a sign needs to achieve in order to be output.
The model outputs ``Unknown Sign'' if the threshold is not met.

\section{Sign Recorder}
\label{sec:sign_recorder}

The \verb|SignRecorder| class contains all the methods which bind the program for real-time detection together. It establishes the required pipeline and uses the methods and classes from other scripts to yield a prediction from a recorded sign. An overview of the pipeline is given below:

\begin{enumerate}
    \item Initialise \verb|SignRecorder| object with an empty list, a DataFrame passed by reference, and a desired sequence length to determine how many frames to record.
    \item Start recording using the \verb|record| method. A boolean \verb|is_recording| is used to determine when the object is recording. The distances in the DataFrame are set to 0.
    \item While recording, a frame from the input is processed and the results are appended to to the list. This is done while the number of frames processed is less than the sequence length. If this exceeds the sequence length, then recording is stopped and the array is converted to a \verb|sign_model| object.
    \item The \verb|sign_model| object is compared with each \verb|sign_model| object in the DataFrame and the resulting distance is stored in the corresponding row.
    \item Once all \verb|sign_model| object in the DataFrame have been compared, the DataFrame is sorted in ascending order, starting with the lowest distances at the top.
    \item The top \verb|batch_size| rows are chosen and the output label is determined as described in Section \ref{sec:batch}.
\end{enumerate}

\section{Sign Evaluator}
\label{sec:sign_eval}

The \verb|SignEval| class contains all the methods which enables the model to be tested on testing data. It uses a similar pipeline to that used in the \verb|sign_recorder| class. One difference is that there is no sequence length passed as parameter, as the length of the video sequences are used instead. There is also no variables and methods associated with recording required. The pipeline can be outlined as follows:

\begin{enumerate}
    \item Initialise \verb|SignEval| object with an empty list, and a DataFrame passed by reference.
    \item While the video has frames, the list is filled with the results of each processed frame.
    \item When the video has no more frames left, the list is used to initialise a  \verb|sign_model| object.
    \item The \verb|sign_model| object is compared with each \verb|sign_model| object in the DataFrame and the resulting distance is stored in the corresponding row.
    \item Once all \verb|sign_model| object in the DataFrame have been compared, the DataFrame is sorted in ascending order, starting with the lowest distances at the top.
    \item The top \verb|batch_size| rows are chosen and the output label is determined as described in Section \ref{sec:batch}.
\end{enumerate}

The \verb|sign_eval| object can be used to generate a list of predicted labels, which can be compared with the list of actual labels to obtain a measure of accuracy.


\section{Webcam Manager}
\label{sec:webcam}

The \verb|WebcamManager| class contains the methods to control the visual interface of the OpenCV feed. It provides information such as what is the output label and when the model is recording. This class is only relevant for real-time detection.

\section{Usage}

The implementation provides a record button to start a capture of a specific number of frames. Once pressed, the feed will capture that amount of frames and process them. To indicate that the feed is recording, a circle is used as an indicator. It turns red while the feed is recording.

A hand and pose model will be generated for each frame and they are all stored in a sign model. Once compared, the program will output the most probable label as a prediction, which will be displayed on the bottom of the feed.

\subsection{Constants}
\label{sec:constants}

\verb|constants.py| is a script containing parameters and constants.
\verb|FASTDTW| is a boolean which toggles between using the basic DTW algorithm and the FastDTW implementation.
\verb|BATCH_SIZE| is an integer representing the batch size mentionned in Section \ref{sec:batch}. This can be tweaked according to the data being trained on.
\verb|DATA_PATH|, \verb|TEST_PATH|, and \verb|FEATURE_PATH| are constants which dictate where training videos, test videos, and keypoint data are stored.
\verb|mappings| is a dictionary which is used to process the LSA64 dataset, associating its corresponding labels by id.


\end{document}